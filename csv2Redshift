import boto3
import pandas as pd
from io import StringIO, BytesIO
import logging
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple
import time

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def clean_column_name(column: str) -> str:
    """Make column names Redshift-friendly"""
    if not column:
        return "unnamed_column"
    cleaned = ''.join(c if c.isalnum() else '_' for c in str(column).lower())
    if cleaned[0].isdigit():
        cleaned = 'col_' + cleaned
    return cleaned

def detect_data_type(values: pd.Series) -> str:
    """Detect the most appropriate Redshift data type based on column values"""
    try:
        non_null_values = values.dropna()
        if len(non_null_values) == 0:
            return 'VARCHAR(256)'
        
        if pd.api.types.is_datetime64_any_dtype(values):
            return 'TIMESTAMP'
        
        if pd.api.types.is_bool_dtype(values):
            return 'BOOLEAN'
            
        if pd.api.types.is_integer_dtype(values):
            return 'BIGINT'
                
        if pd.api.types.is_float_dtype(values):
            return 'DECIMAL(18,2)'
            
        if pd.api.types.is_string_dtype(values):
            max_length = values.astype(str).str.len().max()
            varchar_length = min(max(max_length * 2, 256), 65535)
            return f'VARCHAR({varchar_length})'
            
        return 'VARCHAR(256)'
        
    except Exception as e:
        logger.error(f"Error detecting data type: {str(e)}")
        return 'VARCHAR(256)'

def wait_for_query_completion(redshift_data: boto3.client, statement_id: str, expect_results: bool = True) -> Dict:
    """Wait for a Redshift query to complete and return results if expected"""
    logger.info(f"Waiting for query {statement_id} to complete...")
    while True:
        status_response = redshift_data.describe_statement(Id=statement_id)
        status = status_response['Status']
        logger.info(f"Query status: {status}")
        
        if status == 'FINISHED':
            if expect_results:
                try:
                    result = redshift_data.get_statement_result(Id=statement_id)
                    logger.info("Query results received")
                    return result
                except redshift_data.exceptions.ResourceNotFoundException:
                    logger.info("Query completed but no results to fetch")
                    return None
            return None
        elif status in ['FAILED', 'ABORTED']:
            error_message = status_response.get('Error', 'No error message provided')
            raise Exception(f"Query failed with status {status}: {error_message}")
        
        time.sleep(5)

def check_table_exists(redshift_data: boto3.client, table_name: str) -> bool:
    """Check if table exists and return true/false"""
    try:
        sql = f"""
        SELECT COUNT(*) 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name = '{table_name}';
        """
        
        response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=sql
        )
        
        result = wait_for_query_completion(redshift_data, response['Id'])
        if result and result.get('Records'):
            count = int(result['Records'][0][0]['longValue'])
            return count > 0
        return False
        
    except Exception as e:
        logger.error(f"Error checking table existence: {str(e)}")
        return False

def create_table(redshift_data: boto3.client, table_name: str, df: pd.DataFrame) -> None:
    """Create table in Redshift based on DataFrame schema"""
    try:
        columns = {clean_column_name(col): detect_data_type(df[col]) 
                  for col in df.columns}
        
        column_definitions = [f'"{col}" {dtype}' for col, dtype in columns.items()]
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {', '.join(column_definitions)}
        );
        """
        
        logger.info(f"Creating table with SQL: {create_table_sql}")
        
        response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=create_table_sql
        )
        
        wait_for_query_completion(redshift_data, response['Id'], expect_results=False)
        logger.info(f"Table {table_name} created successfully")
        
    except Exception as e:
        logger.error(f"Error creating table: {str(e)}")
        raise

def load_data_to_redshift(redshift_data: boto3.client, table_name: str, bucket: str, key: str) -> None:
    """Load data from S3 to Redshift using COPY command"""
    try:
        copy_command = f"""
        COPY {table_name}
        FROM 's3://{bucket}/{key}'
        IAM_ROLE '{os.environ['REDSHIFT_ROLE_ARN']}'
        CSV
        IGNOREHEADER 1
        FILLRECORD
        ACCEPTINVCHARS
        MAXERROR 100;
        """
        
        logger.info(f"Executing COPY command: {copy_command}")
        
        copy_response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=copy_command
        )
        
        wait_for_query_completion(redshift_data, copy_response['Id'], expect_results=False)
        
        # Verify data was loaded
        verify_sql = f"SELECT COUNT(*) FROM {table_name};"
        verify_response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=verify_sql
        )
        
        result = wait_for_query_completion(redshift_data, verify_response['Id'])
        if result and result.get('Records'):
            row_count = int(result['Records'][0][0]['longValue'])
            logger.info(f"Loaded {row_count} rows into {table_name}")
            
            if row_count == 0:
                raise Exception(f"No data was loaded into {table_name}")
        else:
            raise Exception("Could not verify data load")
            
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        raise

def move_to_completed(s3_client: boto3.client, bucket: str, source_key: str) -> None:
    """Move processed file to completed folder"""
    try:
        completed_key = source_key.replace('processed/', 'completed/')
        
        s3_client.copy_object(
            CopySource={'Bucket': bucket, 'Key': source_key},
            Bucket=bucket,
            Key=completed_key
        )
        
        s3_client.delete_object(Bucket=bucket, Key=source_key)
        logger.info(f"Moved {source_key} to {completed_key}")
        
    except Exception as e:
        logger.error(f"Error moving file to completed: {str(e)}")
        raise

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    try:
        s3 = boto3.client('s3')
        redshift_data = boto3.client('redshift-data')
        
        bucket = event['Records'][0]['s3']['bucket']['name']
        key = event['Records'][0]['s3']['object']['key']
        
        logger.info(f"Processing file {key} from bucket {bucket}")
        
        try:
            response = s3.get_object(Bucket=bucket, Key=key)
            csv_content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(csv_content))
            logger.info(f"Successfully read CSV with {len(df)} rows")
            
            file_name = os.path.splitext(os.path.basename(key))[0]
            table_name = clean_column_name(file_name)
            
            if not check_table_exists(redshift_data, table_name):
                create_table(redshift_data, table_name, df)
            
            load_data_to_redshift(redshift_data, table_name, bucket, key)
            move_to_completed(s3, bucket, key)
            
            return {
                'statusCode': 200,
                'body': f'Successfully processed {key} into table {table_name}'
            }
            
        except Exception as e:
            logger.error(f"Error processing file {key}: {str(e)}")
            raise
            
    except Exception as e:
        logger.error(f"Error in lambda_handler: {str(e)}")
        return {
            'statusCode': 500,
            'body': f'Error processing file: {str(e)}'
        }