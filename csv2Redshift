import boto3
import pandas as pd
from io import StringIO, BytesIO
import logging
import os
from datetime import datetime
from typing import Dict, List, Any, Tuple

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def clean_column_name(column: str) -> str:
    """Make column names Redshift-friendly"""
    if not column:
        return "unnamed_column"
    cleaned = ''.join(c if c.isalnum() else '_' for c in str(column).lower())
    # Handle columns that start with numbers
    if cleaned[0].isdigit():
        cleaned = 'col_' + cleaned
    return cleaned

def detect_data_type(values: pd.Series) -> str:
    """
    Detect the most appropriate Redshift data type based on column values
    """
    try:
        # Remove null values for type detection
        non_null_values = values.dropna()
        if len(non_null_values) == 0:
            return 'VARCHAR(256)'  # Default for empty columns
        
        # Check if datetime
        if pd.api.types.is_datetime64_any_dtype(values):
            return 'TIMESTAMP'
        
        # Check if boolean
        if pd.api.types.is_bool_dtype(values):
            return 'BOOLEAN'
            
        # Check if integer
        if pd.api.types.is_integer_dtype(values):
            if values.min() >= -32768 and values.max() <= 32767:
                return 'SMALLINT'
            elif values.min() >= -2147483648 and values.max() <= 2147483647:
                return 'INTEGER'
            else:
                return 'BIGINT'
                
        # Check if float
        if pd.api.types.is_float_dtype(values):
            return 'DECIMAL(18,2)'
            
        # For text, determine appropriate VARCHAR length
        if pd.api.types.is_string_dtype(values):
            max_length = values.astype(str).str.len().max()
            # Add buffer for safety
            varchar_length = min(max(max_length * 2, 256), 65535)
            return f'VARCHAR({varchar_length})'
            
        # Default to VARCHAR if type is unknown
        return 'VARCHAR(256)'
        
    except Exception as e:
        logger.error(f"Error detecting data type: {str(e)}")
        return 'VARCHAR(256)'  # Safe default

def check_redshift_table_exists(redshift_data: boto3.client, 
                              table_name: str) -> bool:
    """Check if table exists in Redshift"""
    try:
        sql = f"""
        SELECT EXISTS (
            SELECT 1 
            FROM information_schema.tables 
            WHERE table_name = '{table_name}'
        );
        """
        response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=sql
        )
        logger.info(f"Table existence check response: {response}")
        return True
    except Exception as e:
        logger.info(f"Table {table_name} does not exist: {str(e)}")
        return False

def create_table(redshift_data: boto3.client,
                table_name: str,
                df: pd.DataFrame) -> None:
    """Create table in Redshift based on DataFrame schema"""
    try:
        # Clean column names and detect types
        columns = {clean_column_name(col): detect_data_type(df[col]) 
                  for col in df.columns}
        
        # Generate CREATE TABLE SQL
        column_definitions = [f'"{col}" {dtype}' for col, dtype in columns.items()]
        create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            {', '.join(column_definitions)}
        );
        """
        
        logger.info(f"Creating table with SQL: {create_table_sql}")
        
        # Execute CREATE TABLE
        response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=create_table_sql
        )
        logger.info(f"Create table response: {response}")
        
    except Exception as e:
        logger.error(f"Error creating table: {str(e)}")
        raise

def load_data_to_redshift(redshift_data: boto3.client,
                         s3_client: boto3.client,
                         table_name: str,
                         bucket: str,
                         key: str) -> None:
    """Load data from S3 to Redshift using COPY command"""
    try:
        # Construct COPY command
        copy_command = f"""
        COPY {table_name}
        FROM 's3://{bucket}/{key}'
        IAM_ROLE '{os.environ['REDSHIFT_ROLE_ARN']}'
        CSV
        IGNOREHEADER 1
        FILLRECORD
        ACCEPTINVCHARS
        TRUNCATECOLUMNS
        MAXERROR 100;
        """
        
        logger.info(f"Executing COPY command: {copy_command}")
        
        # Execute COPY command
        copy_response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=copy_command
        )
        logger.info(f"COPY command response: {copy_response}")
        
        # Check for load errors
        check_errors_sql = "SELECT * FROM stl_load_errors ORDER BY starttime DESC LIMIT 5;"
        error_response = redshift_data.execute_statement(
            Database=os.environ['REDSHIFT_DATABASE'],
            WorkgroupName=os.environ['REDSHIFT_WORKGROUP'],
            Sql=check_errors_sql
        )
        logger.info(f"Load errors check response: {error_response}")
        
    except Exception as e:
        logger.error(f"Error loading data: {str(e)}")
        raise

def move_to_completed(s3_client: boto3.client,
                     bucket: str,
                     source_key: str) -> None:
    """Move processed file to completed folder"""
    try:
        # Generate new key for completed folder
        completed_key = source_key.replace('processed/', 'completed/')
        
        # Copy to completed folder
        s3_client.copy_object(
            CopySource={'Bucket': bucket, 'Key': source_key},
            Bucket=bucket,
            Key=completed_key
        )
        
        # Delete from processed folder
        s3_client.delete_object(Bucket=bucket, Key=source_key)
        
        logger.info(f"Moved {source_key} to {completed_key}")
        
    except Exception as e:
        logger.error(f"Error moving file to completed: {str(e)}")
        raise

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    try:
        # Initialize AWS clients
        s3 = boto3.client('s3')
        redshift_data = boto3.client('redshift-data')
        
        # Get the triggering file details
        bucket = event['Records'][0]['s3']['bucket']['name']
        key = event['Records'][0]['s3']['object']['key']
        
        logger.info(f"Processing file {key} from bucket {bucket}")
        
        try:
            # Read CSV file
            response = s3.get_object(Bucket=bucket, Key=key)
            csv_content = response['Body'].read().decode('utf-8')
            df = pd.read_csv(StringIO(csv_content))
            logger.info(f"Successfully read CSV with {len(df)} rows")
            
            # Generate table name from file name
            file_name = os.path.splitext(os.path.basename(key))[0]
            table_name = clean_column_name(file_name)
            
            # Create table if it doesn't exist
            if not check_redshift_table_exists(redshift_data, table_name):
                create_table(redshift_data, table_name, df)
            
            # Load data
            load_data_to_redshift(redshift_data, s3, table_name, bucket, key)
            
            # Move file to completed folder
            move_to_completed(s3, bucket, key)
            
            return {
                'statusCode': 200,
                'body': f'Successfully processed {key} into table {table_name}'
            }
            
        except Exception as e:
            logger.error(f"Error processing file {key}: {str(e)}")
            raise
            
    except Exception as e:
        logger.error(f"Error in lambda_handler: {str(e)}")
        return {
            'statusCode': 500,
            'body': f'Error processing file: {str(e)}'
        }